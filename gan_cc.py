# -*- coding: utf-8 -*-
"""GAN_CC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15VfG0S9pLevvXONT1ORotf-oN8R7Ps6G
"""

import numpy as np
import pandas as pd
from tensorflow.keras.layers import Input, Dense, LeakyReLU
from tensorflow.keras.models import Model, Sequential
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np

data = pd.read_csv("D:\InsightAI\creditcard.csv")

data.shape

subset_data = data.iloc[:1000]

subset_data.shape

generator = tf.keras.Sequential([
    layers.Dense(128, activation='relu', input_dim=100),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
])

discriminator = tf.keras.Sequential([
    layers.Dense(64, activation='relu', input_dim=1),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

cross_entropy = tf.keras.losses.BinaryCrossentropy()

#Define optimizers
generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)

#1

def train_gan(dataset, epochs, generator, discriminator, gen_optimizer, disc_optimizer):
    for epoch in range(epochs):
        for real_transactions, _ in dataset:
            noise = tf.random.normal([len(real_transactions), 100])

            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                generated_transactions = generator(noise, training=True)
                real_output = discriminator(real_transactions, training=True)
                generated_output = discriminator(generated_transactions, training=True)

                gen_loss = cross_entropy(tf.ones_like(generated_output), generated_output)
                disc_loss = cross_entropy(tf.ones_like(real_output), real_output) + cross_entropy(tf.zeros_like(generated_output), generated_output)

            gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
            gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

            gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
            disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

            # # Optional: Print or log loss values for monitoring during training
            # if epoch % 10 == 0:
            #     print(f'Epoch: {epoch}, Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}')



transactions = pd.DataFrame(transaction_amounts)
labels_data = pd.DataFrame(labels)
print(transactions.shape)
print(labels.shape)

dataset = tf.data.Dataset.from_tensor_slices((normalized_amounts, labels)).batch(32)

# train_gan(dataset, epochs=10)

# Train the GAN on the credit card transaction dataset
train_gan(dataset, epochs=50, generator=generator, discriminator=discriminator,
          gen_optimizer=generator_optimizer, disc_optimizer=discriminator_optimizer)

num_synthetic_transactions = len(transaction_amounts)
noise = tf.random.normal([num_synthetic_transactions, 100])
synthetic_amounts = generator(noise, training=False)
synthetic_transaction_amounts = synthetic_amounts * (np.max(transaction_amounts) - np.min(transaction_amounts)) + np.min(transaction_amounts)

# num_synthetic_transactions = 1000
# noise = tf.random.normal([num_synthetic_transactions, 100])
# synthetic_amounts = generator(noise, training=False)

transaction_amounts

synthetic_transaction_amounts

# Set a percentile-based threshold for potential fraud detection
threshold_percentile = 30# Adjust this as needed

# Calculate the absolute deviations between synthetic and real transaction amounts
deviations = np.abs(synthetic_transaction_amounts - transaction_amounts)

# Calculate the threshold based on the chosen percentile
threshold_value = np.percentile(deviations, threshold_percentile)

# Find potential fraud indices based on the threshold
potential_fraud_indices = np.where(deviations > threshold_value)[0]

if len(potential_fraud_indices) > 0:
    print("Potential fraud detected for the following transactions:")
    for idx in potential_fraud_indices:
        print(f"Transaction {idx + 1}: Deviation = {deviations[idx]}")
else:
    print("No potential fraud detected.")

# Compare real transactions with synthetic transactions
threshold = threshold_value
real_labels = np.ones(len(transaction_amounts))
generated_labels = np.zeros(len(synthetic_transaction_amounts))
all_labels = np.concatenate([real_labels, generated_labels])
all_transactions = np.concatenate([transaction_amounts.flatten(), synthetic_transaction_amounts.numpy().flatten()])

# Create confusion matrix
conf_matrix = confusion_matrix(all_labels, all_transactions > threshold)
print("Confusion Matrix:")
print(conf_matrix)

